{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#张量Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "data = [ 1, 2],[3, 4]\n",
    "x_data = torch.tensor(data)\n",
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "x_np: tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "#从 NumPy 数组\n",
    "np_array = np.array(data)\n",
    "print(type(np_array))\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(\"x_np:\",x_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.0800, 0.5594],\n",
      "        [0.5067, 0.9433]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data) # 保留of x_data的属性\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # 覆盖 x_data的数据类型\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.8557, 0.9500, 0.4287, 0.0974],\n",
      "        [0.2706, 0.8924, 0.8717, 0.8071],\n",
      "        [0.0472, 0.9018, 0.3577, 0.1999]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "shape = (3,4)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0276, 0.7917, 0.0796],\n",
      "        [0.0345, 0.3727, 0.9845],\n",
      "        [0.8195, 0.9291, 0.6054],\n",
      "        [0.8002, 0.8742, 0.5369],\n",
      "        [0.2782, 0.5006, 0.0310]]) \n",
      " tensor([[-0.7239, -2.7178, -1.7371],\n",
      "        [ 0.2846, -0.4810,  0.2214],\n",
      "        [ 0.5286,  1.4332, -1.4952],\n",
      "        [-1.6578, -1.0025,  0.6062],\n",
      "        [-0.7887,  0.1396, -2.9089]]) \n",
      " tensor([[ 0.2358,  0.2369,  0.6932],\n",
      "        [-0.3561, -1.0838,  0.8161],\n",
      "        [-1.8045,  1.8848,  0.3255],\n",
      "        [-0.2777,  0.3081, -0.3373],\n",
      "        [-0.2259, -0.6845,  0.2479]]) \n",
      " tensor([ 1.0000,  1.4737,  1.9474,  2.4211,  2.8947,  3.3684,  3.8421,  4.3158,\n",
      "         4.7895,  5.2632,  5.7368,  6.2105,  6.6842,  7.1579,  7.6316,  8.1053,\n",
      "         8.5789,  9.0526,  9.5263, 10.0000])\n"
     ]
    }
   ],
   "source": [
    "# 基于现有tensor构建，但使⽤新值填充\n",
    "m = torch.ones(5,3, dtype=torch.double)\n",
    "n = torch.rand_like(m, dtype=torch.float)\n",
    "# 获取tensor的⼤⼩\n",
    "# print(m.size()) # torch.Size([5,3])\n",
    "# print(m.type())\n",
    "# print(n.size()) \n",
    "# print(n.type())\n",
    "\n",
    "# 均匀分布\n",
    "a1=torch.rand(5,3)\n",
    "# 标准正态分布\n",
    "a2=torch.randn(5,3)\n",
    "# 离散正态分布\n",
    "a3=torch.normal(mean=.0,std=1.0,size=(5,3))\n",
    "# 线性间隔向量(返回⼀个1维张量，包含在区间start和end上均匀间隔的steps个点)\n",
    "a4=torch.linspace(start=1,end=10,steps=20)\n",
    "print(a1,\"\\n\",a2,\"\\n\",a3,\"\\n\",a4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- 张量的属性\n",
    "张量的属性描述了张量的形状、数据类型和存储它们的设备。以对象的⻆度来判断，\n",
    "张量可以看做是具有特征和⽅法的对象。 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS 可用，可以使用 M 系列芯片的 GPU\n"
     ]
    }
   ],
   "source": [
    "# 设置张量在GPU上运算\n",
    "if torch.cuda.is_available():\n",
    " tensor = tensor.to('cuda')\n",
    "# 检查 MPS 是否可用\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    print(\"MPS 可用，可以使用 M 系列芯片的 GPU\")\n",
    "else:\n",
    "    print(\"MPS 不可用，只能使用 CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row:  tensor([1., 1., 1., 1.])\n",
      "First column:  tensor([1., 1., 1., 1.])\n",
      "Last column: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "#张量的索引和切⽚：\n",
    "tensor = torch.ones(4, 4)\n",
    "print('First row: ', tensor[0])\n",
    "print('First column: ', tensor[:, 0])\n",
    "#这里...理解不是特别透彻，目前理解:是做切片操作，...是维度操作，在二维的张量下结果一样，但是更高维度结果不一样\n",
    "print('Last column:', tensor[..., -1])\n",
    "#对所有行的，index为1的列操作 \n",
    "# : 表示选取第一维的所有元素，-1 表示在第二维上选取最后一个元素\n",
    "#... 会代表前面所有的维度\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n",
      "tensor([[[1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.]],\n",
      "\n",
      "        [[1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.]],\n",
      "\n",
      "        [[1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.]],\n",
      "\n",
      "        [[1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "# 可以使⽤ torch.cat ⽤来连接指定维度的⼀系列张量。另⼀个和 torch.cat 功能类似的函数\n",
    "# 是torch.stack\n",
    "#tensor原本是4,4 dim=1在第一个维度上拼接，则拼接为4,12\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)\n",
    "#stack是张量堆叠，原本4,4 3个张量在第一个维度上堆叠后为4,3,4\n",
    "t1 = torch.stack([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "tensorT$tensor([[1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "$tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3., 3., 3.],\n",
       "        [3., 3., 3., 3.],\n",
       "        [3., 3., 3., 3.],\n",
       "        [3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算两个张量之间矩阵乘法的⼏种⽅式。 y1, y2, y3 最后的值是⼀样的 dot\n",
    "print(f\"${tensor}\")\n",
    "print(f\"tensorT${tensor.T}\")\n",
    "y1 = tensor @ tensor.T\n",
    "print(f\"${y1}\")\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "y3 = torch.rand_like(tensor)\n",
    "torch.matmul(tensor, tensor.T, out=y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算张量逐元素相乘的⼏种⽅法。 z1, z2, z3 最后的值是⼀样的。\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "12.0\n",
      "<class 'torch.Tensor'>\n",
      "12.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "# 单元素张量\n",
    "# 如果⼀个单元素张量，例如将张量的值聚合计算，可以使⽤ item() ⽅法将其转换为\n",
    "# Python 数值\n",
    "print(f\"{tensor}\")\n",
    "agg = tensor.sum()\n",
    "print(f\"{agg}\")\n",
    "print(type(agg))\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[11., 10., 11., 11.],\n",
      "        [11., 10., 11., 11.],\n",
      "        [11., 10., 11., 11.],\n",
      "        [11., 10., 11., 11.]])\n"
     ]
    }
   ],
   "source": [
    "# In-place操作\n",
    "# 把计算结果存储到当前操作数中的操作就称为就地操作。含义和pandas中inPlace参\n",
    "# 数的含义⼀样。pytorch中，这些操作是由带有下划线 _ 后缀的函数表⽰。例如：\n",
    "# x.copy_(y) ,  x.t_() , 将改变 x ⾃⾝的值。\n",
    "print(tensor, \"\\n\")\n",
    "tensor.add_(10)\n",
    "print(tensor)\n",
    "#这方法执行后竟然相当于全局变量，每执行一次都在累加。与numpy之间的转换CPU 和 NumPy 数组上的张量共享底层内存位置，所以改变⼀个另⼀个也会变。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n",
      "s: tensor([1., 1., 1., 1., 1.])\n",
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n",
      "n: tensor([2., 2., 2., 2., 2.])\n"
     ]
    }
   ],
   "source": [
    "#张量到numpy数组\n",
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")\n",
    "#数组到张量\n",
    "s=torch.from_numpy(n)\n",
    "print(f\"s: {s}\")\n",
    "#NumPy 数组的变化也反映在张量中。\n",
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")\n",
    "print(f\"n: {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j7/jk94b4xn57g1p5gbnv0z33080000gp/T/ipykernel_3044/4028248882.py:9: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3729.)\n",
      "  result = torch.matmul(A, x.T) + torch.matmul(b, x) + c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'expression2.png'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "# 定义矩阵 A，向量 b 和常数 c\n",
    "#requires_grad（可选）：一个布尔值，用于指定是否需要对该张量进行梯度计算，默认值为 False\n",
    "A = torch.randn(10, 10,requires_grad=True)\n",
    "b = torch.randn(10,requires_grad=True)\n",
    "c = torch.randn(1,requires_grad=True)\n",
    "x = torch.randn(10, requires_grad=True)\n",
    "# 计算 x^T * A + b * x + c\n",
    "result = torch.matmul(A, x.T) + torch.matmul(b, x) + c\n",
    "print(type(result))\n",
    "# ⽣成计算图节点\n",
    "dot = make_dot(result, params={'A': A, 'b': b, 'c': c, 'x': x})\n",
    "# 绘制计算图\n",
    "dot.render('expression', format='png', cleanup=True, view=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "circle-py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
