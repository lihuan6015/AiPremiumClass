{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于KMNIST数据集实现神经网络分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader  # 数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "#设置超参数\n",
    "LR = 3e-4\n",
    "epochs = 200\n",
    "BATCH_SIZE = 512\n",
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#数据预处理\n",
    "train_data=datasets.KMNIST(root=\"./data\",train=True,download=True,transform=ToTensor())\n",
    "test_data=datasets.KMNIST(root=\"./data\",train=False,download=True,transform=ToTensor())\n",
    "trian_dl = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)#,num_workers=0\n",
    "test_dl = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# clzzs=set([clzz for img,clzz in train_data])\n",
    "# print(clzzs)\n",
    "# plt.imshow(train_data[1][0])\n",
    "# plt.show()\n",
    "# train_data[1][0]\n",
    "# train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型构建（神经网络层、损失函数、优化器）\n",
    "model=nn.Sequential(\n",
    "    nn.Linear(28*28,1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5), \n",
    "    nn.Linear(1024,512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),        # 防止过拟合\n",
    "    nn.Linear(512,256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),        # 防止过拟合\n",
    "    nn.Linear(256,128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128,64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Linear(64,10),\n",
    ").to(DEVICE)\n",
    "#交叉熵损失函数\n",
    "loss_fn=nn.CrossEntropyLoss()\n",
    "#优化器随机梯度下降，（Stochastic Gradient Descent）；每次从数据集中随机抽取一个（或一小批）样本计算梯度，并更新参数。\n",
    "#optimizer=torch.optim.SGD(model.parameters(),lr=LR)\n",
    "#自适应学习率，收敛速度快，适合大多数任务。对超参数鲁棒（学习率通常只需设置为3e-4）。可能在某些任务上不如调优后的SGD泛化性能好。\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss=0.6570, Val Acc=64.53%\n",
      "Epoch:0 Loss: 0.6569967865943909\n",
      "Epoch 1: Loss=0.5611, Val Acc=75.88%\n",
      "Epoch:1 Loss: 0.561084508895874\n",
      "Epoch 2: Loss=0.2911, Val Acc=81.18%\n",
      "Epoch:2 Loss: 0.2911463677883148\n",
      "Epoch 3: Loss=0.3439, Val Acc=84.33%\n",
      "Epoch:3 Loss: 0.3439289629459381\n",
      "Epoch 4: Loss=0.2146, Val Acc=86.31%\n",
      "Epoch:4 Loss: 0.21455204486846924\n",
      "Epoch 5: Loss=0.2102, Val Acc=87.58%\n",
      "Epoch:5 Loss: 0.2101944237947464\n",
      "Epoch 6: Loss=0.2652, Val Acc=88.98%\n",
      "Epoch:6 Loss: 0.26515865325927734\n",
      "Epoch 7: Loss=0.1591, Val Acc=89.50%\n",
      "Epoch:7 Loss: 0.15912383794784546\n",
      "Epoch 8: Loss=0.1554, Val Acc=89.28%\n",
      "Epoch:8 Loss: 0.1553899496793747\n",
      "Epoch 9: Loss=0.1573, Val Acc=90.08%\n",
      "Epoch:9 Loss: 0.1572674959897995\n",
      "Epoch 10: Loss=0.0888, Val Acc=90.68%\n",
      "Epoch:10 Loss: 0.08877026289701462\n",
      "Epoch 11: Loss=0.0865, Val Acc=90.77%\n",
      "Epoch:11 Loss: 0.08650527149438858\n",
      "Epoch 12: Loss=0.0673, Val Acc=91.17%\n",
      "Epoch:12 Loss: 0.06726304441690445\n",
      "Epoch 13: Loss=0.0753, Val Acc=91.12%\n",
      "Epoch:13 Loss: 0.07532978057861328\n",
      "Epoch 14: Loss=0.0215, Val Acc=91.14%\n",
      "Epoch:14 Loss: 0.021536007523536682\n",
      "Epoch 15: Loss=0.0098, Val Acc=91.43%\n",
      "Epoch:15 Loss: 0.009775501675903797\n",
      "Epoch 16: Loss=0.1051, Val Acc=91.62%\n",
      "Epoch:16 Loss: 0.10512008517980576\n",
      "Epoch 17: Loss=0.0688, Val Acc=91.03%\n",
      "Epoch:17 Loss: 0.06881177425384521\n",
      "Epoch 18: Loss=0.1273, Val Acc=91.81%\n",
      "Epoch:18 Loss: 0.12728430330753326\n",
      "Epoch 19: Loss=0.1390, Val Acc=91.80%\n",
      "Epoch:19 Loss: 0.13901536166667938\n",
      "Epoch 20: Loss=0.0355, Val Acc=91.84%\n",
      "Epoch:20 Loss: 0.0355425588786602\n",
      "Epoch 21: Loss=0.0583, Val Acc=91.80%\n",
      "Epoch:21 Loss: 0.058337945491075516\n",
      "Epoch 22: Loss=0.0969, Val Acc=92.00%\n",
      "Epoch:22 Loss: 0.09692361205816269\n",
      "Epoch 23: Loss=0.0413, Val Acc=91.80%\n",
      "Epoch:23 Loss: 0.04127952829003334\n",
      "Epoch 24: Loss=0.0327, Val Acc=92.21%\n",
      "Epoch:24 Loss: 0.0326940082013607\n",
      "Epoch 25: Loss=0.0853, Val Acc=91.91%\n",
      "Epoch:25 Loss: 0.08533277362585068\n",
      "Epoch 26: Loss=0.0288, Val Acc=92.11%\n",
      "Epoch:26 Loss: 0.028832992538809776\n",
      "Epoch 27: Loss=0.0178, Val Acc=92.39%\n",
      "Epoch:27 Loss: 0.017812812700867653\n",
      "Epoch 28: Loss=0.0717, Val Acc=92.22%\n",
      "Epoch:28 Loss: 0.07165498286485672\n",
      "Epoch 29: Loss=0.0112, Val Acc=91.91%\n",
      "Epoch:29 Loss: 0.01119739655405283\n",
      "Epoch 30: Loss=0.0395, Val Acc=92.06%\n",
      "Epoch:30 Loss: 0.03950440138578415\n",
      "Epoch 31: Loss=0.0225, Val Acc=92.39%\n",
      "Epoch:31 Loss: 0.022451654076576233\n",
      "Epoch 32: Loss=0.0131, Val Acc=92.09%\n",
      "Epoch:32 Loss: 0.013136826455593109\n",
      "Epoch 33: Loss=0.0038, Val Acc=92.05%\n",
      "Epoch:33 Loss: 0.003773026168346405\n",
      "Epoch 34: Loss=0.0351, Val Acc=91.88%\n",
      "Epoch:34 Loss: 0.03514648973941803\n",
      "Epoch 35: Loss=0.0045, Val Acc=92.20%\n",
      "Epoch:35 Loss: 0.004512757062911987\n",
      "Epoch 36: Loss=0.0057, Val Acc=92.25%\n",
      "Epoch:36 Loss: 0.005709072574973106\n",
      "Epoch 37: Loss=0.0220, Val Acc=92.18%\n",
      "Epoch:37 Loss: 0.021979212760925293\n",
      "Epoch 38: Loss=0.0054, Val Acc=92.13%\n",
      "Epoch:38 Loss: 0.0054312520660459995\n",
      "Epoch 39: Loss=0.0107, Val Acc=92.30%\n",
      "Epoch:39 Loss: 0.010656449012458324\n",
      "Epoch 40: Loss=0.0209, Val Acc=92.32%\n",
      "Epoch:40 Loss: 0.02089034952223301\n",
      "Epoch 41: Loss=0.0677, Val Acc=92.30%\n",
      "Epoch:41 Loss: 0.06771313399076462\n",
      "Epoch 42: Loss=0.0176, Val Acc=92.14%\n",
      "Epoch:42 Loss: 0.017590155825018883\n",
      "Epoch 43: Loss=0.0101, Val Acc=92.20%\n",
      "Epoch:43 Loss: 0.01013494748622179\n",
      "Epoch 44: Loss=0.0458, Val Acc=92.26%\n",
      "Epoch:44 Loss: 0.045760273933410645\n",
      "Epoch 45: Loss=0.0087, Val Acc=92.67%\n",
      "Epoch:45 Loss: 0.008746837265789509\n",
      "Epoch 46: Loss=0.0050, Val Acc=92.31%\n",
      "Epoch:46 Loss: 0.004991667810827494\n",
      "Epoch 47: Loss=0.0069, Val Acc=92.20%\n",
      "Epoch:47 Loss: 0.006858983542770147\n",
      "Epoch 48: Loss=0.0067, Val Acc=92.56%\n",
      "Epoch:48 Loss: 0.006721023935824633\n",
      "Epoch 49: Loss=0.1648, Val Acc=92.26%\n",
      "Epoch:49 Loss: 0.16484372317790985\n",
      "Epoch 50: Loss=0.0154, Val Acc=92.32%\n",
      "Epoch:50 Loss: 0.015398946590721607\n",
      "Epoch 51: Loss=0.0166, Val Acc=92.41%\n",
      "Epoch:51 Loss: 0.016595719382166862\n",
      "Epoch 52: Loss=0.0019, Val Acc=92.33%\n",
      "Epoch:52 Loss: 0.0018788819434121251\n",
      "Epoch 53: Loss=0.0453, Val Acc=92.46%\n",
      "Epoch:53 Loss: 0.04528385400772095\n",
      "Epoch 54: Loss=0.0021, Val Acc=92.34%\n",
      "Epoch:54 Loss: 0.002073639538139105\n",
      "Epoch 55: Loss=0.0157, Val Acc=92.76%\n",
      "Epoch:55 Loss: 0.01571657508611679\n",
      "Epoch 56: Loss=0.0142, Val Acc=92.84%\n",
      "Epoch:56 Loss: 0.014241748489439487\n",
      "Epoch 57: Loss=0.0118, Val Acc=92.90%\n",
      "Epoch:57 Loss: 0.011842004023492336\n",
      "Epoch 58: Loss=0.0058, Val Acc=92.51%\n",
      "Epoch:58 Loss: 0.00580610753968358\n",
      "Epoch 59: Loss=0.0201, Val Acc=92.69%\n",
      "Epoch:59 Loss: 0.02012796513736248\n",
      "Epoch 60: Loss=0.0008, Val Acc=92.93%\n",
      "Epoch:60 Loss: 0.0008431961759924889\n",
      "Epoch 61: Loss=0.0042, Val Acc=92.79%\n",
      "Epoch:61 Loss: 0.004218448419123888\n",
      "Epoch 62: Loss=0.0014, Val Acc=92.63%\n",
      "Epoch:62 Loss: 0.0014357747277244925\n",
      "Epoch 63: Loss=0.0435, Val Acc=92.32%\n",
      "Epoch:63 Loss: 0.04351741448044777\n",
      "Epoch 64: Loss=0.0061, Val Acc=92.40%\n",
      "Epoch:64 Loss: 0.006086375564336777\n",
      "Epoch 65: Loss=0.0232, Val Acc=92.86%\n",
      "Epoch:65 Loss: 0.02317689172923565\n",
      "Epoch 66: Loss=0.0008, Val Acc=92.49%\n",
      "Epoch:66 Loss: 0.0008133184746839106\n",
      "Epoch 67: Loss=0.0001, Val Acc=92.50%\n",
      "Epoch:67 Loss: 0.000115476745122578\n",
      "Epoch 68: Loss=0.0009, Val Acc=92.53%\n",
      "Epoch:68 Loss: 0.0009402164141647518\n",
      "Epoch 69: Loss=0.0166, Val Acc=92.48%\n",
      "Epoch:69 Loss: 0.016638558357954025\n",
      "Epoch 70: Loss=0.0054, Val Acc=92.41%\n",
      "Epoch:70 Loss: 0.005418157204985619\n",
      "Epoch 71: Loss=0.0050, Val Acc=92.79%\n",
      "Epoch:71 Loss: 0.0050072516314685345\n",
      "Epoch 72: Loss=0.0045, Val Acc=92.64%\n",
      "Epoch:72 Loss: 0.004548318684101105\n",
      "Epoch 73: Loss=0.0027, Val Acc=92.52%\n",
      "Epoch:73 Loss: 0.0027111591771245003\n",
      "Epoch 74: Loss=0.0103, Val Acc=92.77%\n",
      "Epoch:74 Loss: 0.010345973074436188\n",
      "Epoch 75: Loss=0.0082, Val Acc=92.89%\n",
      "Epoch:75 Loss: 0.008196343667805195\n",
      "Epoch 76: Loss=0.0206, Val Acc=92.74%\n",
      "Epoch:76 Loss: 0.020618736743927002\n",
      "Epoch 77: Loss=0.0035, Val Acc=92.47%\n",
      "Epoch:77 Loss: 0.003453747369349003\n",
      "Epoch 78: Loss=0.0025, Val Acc=92.69%\n",
      "Epoch:78 Loss: 0.0024619547184556723\n",
      "Epoch 79: Loss=0.0021, Val Acc=92.71%\n",
      "Epoch:79 Loss: 0.002102519618347287\n",
      "Epoch 80: Loss=0.0083, Val Acc=92.66%\n",
      "Epoch:80 Loss: 0.008320610970258713\n",
      "Epoch 81: Loss=0.0045, Val Acc=92.31%\n",
      "Epoch:81 Loss: 0.004450532142072916\n",
      "Epoch 82: Loss=0.0006, Val Acc=92.96%\n",
      "Epoch:82 Loss: 0.0005742830107919872\n",
      "Epoch 83: Loss=0.0124, Val Acc=92.48%\n",
      "Epoch:83 Loss: 0.012366036884486675\n",
      "Epoch 84: Loss=0.0008, Val Acc=92.58%\n",
      "Epoch:84 Loss: 0.000839406915474683\n",
      "Epoch 85: Loss=0.0021, Val Acc=92.53%\n",
      "Epoch:85 Loss: 0.0020725757349282503\n",
      "Epoch 86: Loss=0.0014, Val Acc=93.00%\n",
      "Epoch:86 Loss: 0.0013665902661159635\n",
      "Epoch 87: Loss=0.0794, Val Acc=92.81%\n",
      "Epoch:87 Loss: 0.07943036407232285\n",
      "Epoch 88: Loss=0.0043, Val Acc=92.74%\n",
      "Epoch:88 Loss: 0.0042700995691120625\n",
      "Epoch 89: Loss=0.0014, Val Acc=92.36%\n",
      "Epoch:89 Loss: 0.0014448035508394241\n",
      "Epoch 90: Loss=0.0445, Val Acc=92.53%\n",
      "Epoch:90 Loss: 0.04451873525977135\n",
      "Epoch 91: Loss=0.0300, Val Acc=92.62%\n",
      "Epoch:91 Loss: 0.02998744137585163\n",
      "Epoch 92: Loss=0.0010, Val Acc=92.61%\n",
      "Epoch:92 Loss: 0.0010333043755963445\n",
      "Epoch 93: Loss=0.0017, Val Acc=92.38%\n",
      "Epoch:93 Loss: 0.0017147503094747663\n",
      "Epoch 94: Loss=0.0089, Val Acc=92.68%\n",
      "Epoch:94 Loss: 0.008878697641193867\n",
      "Epoch 95: Loss=0.0004, Val Acc=92.53%\n",
      "Epoch:95 Loss: 0.000357378157787025\n",
      "Epoch 96: Loss=0.0027, Val Acc=92.67%\n",
      "Epoch:96 Loss: 0.00267860502935946\n",
      "Epoch 97: Loss=0.0040, Val Acc=92.32%\n",
      "Epoch:97 Loss: 0.004026206210255623\n",
      "Epoch 98: Loss=0.0037, Val Acc=92.85%\n",
      "Epoch:98 Loss: 0.003674448234960437\n",
      "Epoch 99: Loss=0.0105, Val Acc=92.82%\n",
      "Epoch:99 Loss: 0.010525103658437729\n",
      "Epoch 100: Loss=0.0036, Val Acc=93.07%\n",
      "Epoch:100 Loss: 0.003562443656846881\n",
      "Epoch 101: Loss=0.0038, Val Acc=92.95%\n",
      "Epoch:101 Loss: 0.0038313872646540403\n",
      "Epoch 102: Loss=0.0049, Val Acc=92.73%\n",
      "Epoch:102 Loss: 0.004918945953249931\n",
      "Epoch 103: Loss=0.0079, Val Acc=92.87%\n",
      "Epoch:103 Loss: 0.007890396751463413\n",
      "Epoch 104: Loss=0.0012, Val Acc=92.60%\n",
      "Epoch:104 Loss: 0.0011603962630033493\n",
      "Epoch 105: Loss=0.0512, Val Acc=92.43%\n",
      "Epoch:105 Loss: 0.0511641688644886\n",
      "Epoch 106: Loss=0.0032, Val Acc=92.83%\n",
      "Epoch:106 Loss: 0.003186451504006982\n",
      "Epoch 107: Loss=0.0151, Val Acc=92.88%\n",
      "Epoch:107 Loss: 0.01506724115461111\n",
      "Epoch 108: Loss=0.0137, Val Acc=92.71%\n",
      "Epoch:108 Loss: 0.013714035041630268\n",
      "Epoch 109: Loss=0.0006, Val Acc=92.67%\n",
      "Epoch:109 Loss: 0.00058648461708799\n",
      "Epoch 110: Loss=0.0039, Val Acc=92.90%\n",
      "Epoch:110 Loss: 0.0038629714399576187\n",
      "Epoch 111: Loss=0.0286, Val Acc=92.88%\n",
      "Epoch:111 Loss: 0.028639478608965874\n",
      "Epoch 112: Loss=0.0029, Val Acc=92.79%\n",
      "Epoch:112 Loss: 0.002920445753261447\n",
      "Epoch 113: Loss=0.0048, Val Acc=92.76%\n",
      "Epoch:113 Loss: 0.004775572568178177\n",
      "Epoch 114: Loss=0.0090, Val Acc=92.75%\n",
      "Epoch:114 Loss: 0.008988424204289913\n",
      "Epoch 115: Loss=0.0005, Val Acc=92.52%\n",
      "Epoch:115 Loss: 0.0005382818635553122\n",
      "Epoch 116: Loss=0.0789, Val Acc=92.72%\n",
      "Epoch:116 Loss: 0.0789429172873497\n",
      "Epoch 117: Loss=0.0065, Val Acc=92.78%\n",
      "Epoch:117 Loss: 0.006476800888776779\n",
      "Epoch 118: Loss=0.0025, Val Acc=92.72%\n",
      "Epoch:118 Loss: 0.0025156657211482525\n",
      "Epoch 119: Loss=0.0034, Val Acc=92.72%\n",
      "Epoch:119 Loss: 0.0033577922731637955\n",
      "Epoch 120: Loss=0.0059, Val Acc=92.60%\n",
      "Epoch:120 Loss: 0.0059307343326509\n",
      "Epoch 121: Loss=0.0006, Val Acc=92.44%\n",
      "Epoch:121 Loss: 0.0006396317039616406\n",
      "Epoch 122: Loss=0.0093, Val Acc=92.49%\n",
      "Epoch:122 Loss: 0.00928343366831541\n",
      "Epoch 123: Loss=0.0017, Val Acc=92.81%\n",
      "Epoch:123 Loss: 0.0016627960139885545\n",
      "Epoch 124: Loss=0.0543, Val Acc=92.88%\n",
      "Epoch:124 Loss: 0.05425348877906799\n",
      "Epoch 125: Loss=0.0035, Val Acc=92.71%\n",
      "Epoch:125 Loss: 0.0035102479159832\n",
      "Epoch 126: Loss=0.0174, Val Acc=92.80%\n",
      "Epoch:126 Loss: 0.017356878146529198\n",
      "Epoch 127: Loss=0.0021, Val Acc=92.92%\n",
      "Epoch:127 Loss: 0.002131278859451413\n",
      "Epoch 128: Loss=0.0005, Val Acc=92.57%\n",
      "Epoch:128 Loss: 0.0005452840123325586\n",
      "Epoch 129: Loss=0.0032, Val Acc=92.89%\n",
      "Epoch:129 Loss: 0.0032031352166086435\n",
      "Epoch 130: Loss=0.0006, Val Acc=93.06%\n",
      "Epoch:130 Loss: 0.0006003191228955984\n",
      "Epoch 131: Loss=0.0073, Val Acc=92.90%\n",
      "Epoch:131 Loss: 0.007305363193154335\n",
      "Epoch 132: Loss=0.0195, Val Acc=92.93%\n",
      "Epoch:132 Loss: 0.019491856917738914\n",
      "Epoch 133: Loss=0.0048, Val Acc=92.75%\n",
      "Epoch:133 Loss: 0.004840122535824776\n",
      "Epoch 134: Loss=0.0012, Val Acc=92.91%\n",
      "Epoch:134 Loss: 0.0011992224026471376\n",
      "Epoch 135: Loss=0.0016, Val Acc=92.59%\n",
      "Epoch:135 Loss: 0.0015548438532277942\n",
      "Epoch 136: Loss=0.0032, Val Acc=92.50%\n",
      "Epoch:136 Loss: 0.003191306022927165\n",
      "Epoch 137: Loss=0.0328, Val Acc=92.54%\n",
      "Epoch:137 Loss: 0.032833751291036606\n",
      "Epoch 138: Loss=0.0006, Val Acc=92.71%\n",
      "Epoch:138 Loss: 0.0005934219807386398\n",
      "Epoch 139: Loss=0.0461, Val Acc=92.79%\n",
      "Epoch:139 Loss: 0.04609822854399681\n",
      "Epoch 140: Loss=0.0419, Val Acc=93.08%\n",
      "Epoch:140 Loss: 0.04186514392495155\n",
      "Epoch 141: Loss=0.0017, Val Acc=92.63%\n",
      "Epoch:141 Loss: 0.0017495666397735476\n",
      "Epoch 142: Loss=0.0006, Val Acc=92.63%\n",
      "Epoch:142 Loss: 0.0005805496475659311\n",
      "Epoch 143: Loss=0.0012, Val Acc=93.06%\n",
      "Epoch:143 Loss: 0.0011878629447892308\n",
      "Epoch 144: Loss=0.0008, Val Acc=92.95%\n",
      "Epoch:144 Loss: 0.0008255073917098343\n",
      "Epoch 145: Loss=0.0003, Val Acc=92.93%\n",
      "Epoch:145 Loss: 0.00028889672830700874\n",
      "Epoch 146: Loss=0.0021, Val Acc=93.02%\n",
      "Epoch:146 Loss: 0.002092270180583\n",
      "Epoch 147: Loss=0.0004, Val Acc=93.11%\n",
      "Epoch:147 Loss: 0.00042154130642302334\n",
      "Epoch 148: Loss=0.0016, Val Acc=92.93%\n",
      "Epoch:148 Loss: 0.0016127526760101318\n",
      "Epoch 149: Loss=0.0007, Val Acc=93.24%\n",
      "Epoch:149 Loss: 0.0006977609009481966\n",
      "Epoch 150: Loss=0.0124, Val Acc=92.85%\n",
      "Epoch:150 Loss: 0.012388564646244049\n",
      "Epoch 151: Loss=0.0001, Val Acc=93.10%\n",
      "Epoch:151 Loss: 8.27625990496017e-05\n",
      "Epoch 152: Loss=0.0156, Val Acc=93.24%\n",
      "Epoch:152 Loss: 0.015608604997396469\n",
      "Epoch 153: Loss=0.0157, Val Acc=92.69%\n",
      "Epoch:153 Loss: 0.01573173888027668\n",
      "Epoch 154: Loss=0.0010, Val Acc=92.68%\n",
      "Epoch:154 Loss: 0.0009559229365549982\n",
      "Epoch 155: Loss=0.0006, Val Acc=92.75%\n",
      "Epoch:155 Loss: 0.0005889912717975676\n",
      "Epoch 156: Loss=0.0020, Val Acc=93.02%\n",
      "Epoch:156 Loss: 0.00199461099691689\n",
      "Epoch 157: Loss=0.0151, Val Acc=93.12%\n",
      "Epoch:157 Loss: 0.015147886238992214\n",
      "Epoch 158: Loss=0.0063, Val Acc=93.01%\n",
      "Epoch:158 Loss: 0.0063210842199623585\n",
      "Epoch 159: Loss=0.0002, Val Acc=93.08%\n",
      "Epoch:159 Loss: 0.0001997991930693388\n",
      "Epoch 160: Loss=0.0005, Val Acc=92.95%\n",
      "Epoch:160 Loss: 0.0005168984644114971\n",
      "Epoch 161: Loss=0.0040, Val Acc=92.97%\n",
      "Epoch:161 Loss: 0.0039714244194328785\n",
      "Epoch 162: Loss=0.0013, Val Acc=93.05%\n",
      "Epoch:162 Loss: 0.0013192896731197834\n",
      "Epoch 163: Loss=0.0004, Val Acc=93.02%\n",
      "Epoch:163 Loss: 0.00036501887370832264\n",
      "Epoch 164: Loss=0.0004, Val Acc=92.90%\n",
      "Epoch:164 Loss: 0.00039155842387117445\n",
      "Epoch 165: Loss=0.0039, Val Acc=92.98%\n",
      "Epoch:165 Loss: 0.003929643426090479\n",
      "Epoch 166: Loss=0.0004, Val Acc=92.85%\n",
      "Epoch:166 Loss: 0.0004403488419484347\n",
      "Epoch 167: Loss=0.0001, Val Acc=93.00%\n",
      "Epoch:167 Loss: 0.00014594268577639014\n",
      "Epoch 168: Loss=0.0160, Val Acc=92.79%\n",
      "Epoch:168 Loss: 0.01604587957262993\n",
      "Epoch 169: Loss=0.0271, Val Acc=92.96%\n",
      "Epoch:169 Loss: 0.02713078260421753\n",
      "Epoch 170: Loss=0.0039, Val Acc=92.65%\n",
      "Epoch:170 Loss: 0.00390649912878871\n",
      "Epoch 171: Loss=0.0274, Val Acc=92.71%\n",
      "Epoch:171 Loss: 0.027419529855251312\n",
      "Epoch 172: Loss=0.0009, Val Acc=92.66%\n",
      "Epoch:172 Loss: 0.0008722368511371315\n",
      "Epoch 173: Loss=0.0010, Val Acc=92.88%\n",
      "Epoch:173 Loss: 0.0010234290966764092\n",
      "Epoch 174: Loss=0.0047, Val Acc=93.15%\n",
      "Epoch:174 Loss: 0.0047262427397072315\n",
      "Epoch 175: Loss=0.0014, Val Acc=93.22%\n",
      "Epoch:175 Loss: 0.0014128579059615731\n",
      "Epoch 176: Loss=0.0019, Val Acc=92.90%\n",
      "Epoch:176 Loss: 0.0018633214058354497\n",
      "Epoch 177: Loss=0.0307, Val Acc=93.06%\n",
      "Epoch:177 Loss: 0.030670246109366417\n",
      "Epoch 178: Loss=0.0001, Val Acc=92.89%\n",
      "Epoch:178 Loss: 0.00010668555478332564\n",
      "Epoch 179: Loss=0.0038, Val Acc=93.05%\n",
      "Epoch:179 Loss: 0.003825185587629676\n",
      "Epoch 180: Loss=0.0006, Val Acc=93.35%\n",
      "Epoch:180 Loss: 0.0006369100883603096\n",
      "Epoch 181: Loss=0.0204, Val Acc=92.66%\n",
      "Epoch:181 Loss: 0.02043474093079567\n",
      "Epoch 182: Loss=0.0001, Val Acc=93.14%\n",
      "Epoch:182 Loss: 9.505046182312071e-05\n",
      "Epoch 183: Loss=0.0001, Val Acc=93.04%\n",
      "Epoch:183 Loss: 0.00014760250633116812\n",
      "Epoch 184: Loss=0.0001, Val Acc=92.78%\n",
      "Epoch:184 Loss: 0.00013903608487453312\n",
      "Epoch 185: Loss=0.0024, Val Acc=92.89%\n",
      "Epoch:185 Loss: 0.002411584137007594\n",
      "Epoch 186: Loss=0.0022, Val Acc=92.70%\n",
      "Epoch:186 Loss: 0.0022205072455108166\n",
      "Epoch 187: Loss=0.0003, Val Acc=92.86%\n",
      "Epoch:187 Loss: 0.00030392786720767617\n",
      "Epoch 188: Loss=0.0008, Val Acc=93.12%\n",
      "Epoch:188 Loss: 0.0008107071626000106\n",
      "Epoch 189: Loss=0.0143, Val Acc=93.03%\n",
      "Epoch:189 Loss: 0.014291748404502869\n",
      "Epoch 190: Loss=0.0002, Val Acc=93.24%\n",
      "Epoch:190 Loss: 0.00016823656915221363\n",
      "Epoch 191: Loss=0.0267, Val Acc=92.95%\n",
      "Epoch:191 Loss: 0.026717735454440117\n",
      "Epoch 192: Loss=0.0054, Val Acc=93.25%\n",
      "Epoch:192 Loss: 0.0053605143912136555\n",
      "Epoch 193: Loss=0.0003, Val Acc=92.89%\n",
      "Epoch:193 Loss: 0.0002659300807863474\n",
      "Epoch 194: Loss=0.0016, Val Acc=92.76%\n",
      "Epoch:194 Loss: 0.0016069436678662896\n",
      "Epoch 195: Loss=0.0064, Val Acc=92.88%\n",
      "Epoch:195 Loss: 0.006363639608025551\n",
      "Epoch 196: Loss=0.0014, Val Acc=93.06%\n",
      "Epoch:196 Loss: 0.0013982723467051983\n",
      "Epoch 197: Loss=0.0001, Val Acc=93.08%\n",
      "Epoch:197 Loss: 0.00014401040971279144\n",
      "Epoch 198: Loss=0.0009, Val Acc=93.09%\n",
      "Epoch:198 Loss: 0.0008908423478715122\n",
      "Epoch 199: Loss=0.0573, Val Acc=92.96%\n",
      "Epoch:199 Loss: 0.05733373761177063\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    #模型训练\n",
    "    best_acc = 0.0\n",
    "    model.train()\n",
    "    for img,clzz in trian_dl:\n",
    "        #将输入的图像数据展平（flatten）成一维向量，以便输入全连接层（Dense Layer）\n",
    "        #如果张量是连续的（例如直接从DataLoader加载的原始图像），view() 和 reshape() 效果相同。\n",
    "        #如果张量是非连续的（例如经过转置或切片操作后），必须用 reshape() 或先调用 contiguous()。\n",
    "        img=img.view(img.size(0),-1).to(DEVICE)\n",
    "        #img=img.to(DEVICE)\n",
    "        clzz=clzz.to(DEVICE)\n",
    "        #前向传播\n",
    "        pre_clzz=model.forward(img)\n",
    "        #计算损失\n",
    "        loss=loss_fn(pre_clzz,clzz)\n",
    "        #反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #更新参数\n",
    "        optimizer.step()\n",
    "\n",
    "    # 验证阶段\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for img, clzz in test_dl:\n",
    "            img, clzz = img.view(img.size(0),-1).to(DEVICE), clzz.to(DEVICE)\n",
    "            outputs = model(img)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += clzz.size(0)\n",
    "            correct += predicted.eq(clzz).sum().item()\n",
    "    acc = 100 * correct / total\n",
    "    print(f'Epoch {epoch}: Loss={loss.item():.4f}, Val Acc={acc:.2f}%')\n",
    "    \n",
    "    # 保存最佳模型\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), 'KMNIST_best_model.pth')\n",
    "    \n",
    "    #scheduler.step()\n",
    "    print(f'Epoch:{epoch} Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.96%\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "test_dl = DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():  # 不计算梯度\n",
    "    for data, target in test_dl:\n",
    "        data = data.view(data.size(0),-1).to(DEVICE)  \n",
    "        #data = data.to(DEVICE)\n",
    "        target = target.to(DEVICE) # 标签形状: [batch_size]\n",
    "        #print(target)\n",
    "        output = model(data)  # 数据形状: [batch_size, 10]\n",
    "        #print(output)\n",
    "        _, predicted = torch.max(output, 1)  # 返回每行最大值和索引 （返回张量中沿指定维度的最大值和对应的索引）\n",
    "        total += target.size(0)  # size(0) 等效 shape[0]\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print(f'Accuracy: {correct/total*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####初步结论：从3层全连接网络层增加到5-6，模型评估准确率从80%+上升至约90%，尤其增加dropout丢弃部分参数后，提升明显，随后继续增加层数准确率无明显提升\n",
    "增加训练轮次50->200，准确率从90%上升至92%，提升较小\n",
    "使用固定学习率容易导致梯度下降到一定程度后反复波动，无法继续收敛或者收敛不明显\n",
    "可能由于训练样本分辨率较低原因，暂无法进一步提升准确率。\n",
    "批次大小从256提升至512，在开始几轮训练中梯度明显下降的更快。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "circle-py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
